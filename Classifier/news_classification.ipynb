{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAOXgvnIlbMS",
        "outputId": "bebe5b3a-a260-444c-d341-4dcc379573f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/newsdataset/preprocessed_data.csv\n"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A71d1TUSlbMU",
        "outputId": "80bcae13-bb7d-4282-c861-8a557c38b0a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sent2vec\n",
            "  Downloading sent2vec-0.3.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting transformers (from sent2vec)\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from sent2vec) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sent2vec) (1.23.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from sent2vec) (4.3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from sent2vec) (3.6.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from sent2vec) (7.4.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->sent2vec) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->sent2vec) (6.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (2.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (0.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->sent2vec) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->sent2vec) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->sent2vec) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->sent2vec) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->sent2vec) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->sent2vec) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->sent2vec) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->sent2vec) (17.0.2)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers->sent2vec)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->sent2vec) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->sent2vec) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers->sent2vec)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->sent2vec)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->sent2vec) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->sent2vec) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->sent2vec) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->sent2vec) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->sent2vec) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->sent2vec) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sent2vec\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sent2vec-0.3.0 tokenizers-0.14.1 transformers-4.34.0\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sent2vec\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6KLo3FnJlbMU"
      },
      "outputs": [],
      "source": [
        "from ast import Num\n",
        "import torch\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "# from utils import load_data, sentence_embedding, label_encoding\n",
        "\n",
        "class NewsSentenceLevelClassificationDataset(Dataset):\n",
        "    def __init__(self,data_path,label_path,num,mode):\n",
        "        self.data_path = data_path\n",
        "        self.label_path = label_path\n",
        "        self.mode = mode\n",
        "        self.num = num\n",
        "        #load dataset\n",
        "        title, classes, class_count = load_pickle(self.data_path,self.label_path,self.num,self.mode)\n",
        "        #sentence embedding\n",
        "        #embedded_title = sentence_embedding(title)\n",
        "\n",
        "        #label encoding\n",
        "\n",
        "        #label_encode= label_encoding(classes)\n",
        "        self.data = title\n",
        "        self.classes = classes\n",
        "        self.num_class = class_count\n",
        "        print(len(self.data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        X = self.data[idx]\n",
        "        Y = F.one_hot(torch.tensor(self.classes[idx]), num_classes=self.num_class)\n",
        "        return X, Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Q_OcJ9EdlbMV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from sent2vec.vectorizer import Vectorizer\n",
        "import pickle\n",
        "import sent2vec\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import sys, os\n",
        "\n",
        "def load_pickle(data_path,label_path,num,mode):\n",
        "\n",
        "    data_file = open(data_path,\"rb\")\n",
        "    label_file = open(label_path,\"rb\")\n",
        "    data =pickle.load(data_file)\n",
        "    label = pickle.load(label_file)\n",
        "\n",
        "    class_count =np.unique(label).shape[0]\n",
        "\n",
        "    length =int(len(data) * num)\n",
        "    if mode == \"train\":\n",
        "       return data[:length],label[:length],class_count\n",
        "    elif mode== \"valid\":\n",
        "      length = int(len(data)*(1-num))\n",
        "      return data[length:], label[length:], class_count\n",
        "    else:\n",
        "        return data, label, class_count\n",
        "# def load_data(path,num):\n",
        "#     df =   pd.read_csv(path)\n",
        "#     df = df.dropna()\n",
        "#     df = df.sample(n=num)\n",
        "#     title = list(df['title'])\n",
        "#     num_classes = len(df['class'].unique())\n",
        "#     classes =list(df['class'])\n",
        "#     return title, classes, num_classes\n",
        "\n",
        "# def sentence_embedding(title):\n",
        "#     string = []\n",
        "#     string.append(title)\n",
        "\n",
        "\n",
        "#     #stop_words = list(set(stopwords))\n",
        "#     blockPrint()\n",
        "#     vector_obj =Vectorizer()\n",
        "#     sentence_vector = []\n",
        "#     #print(\"Size of the length\",type(title))\n",
        "#     enablePrint()\n",
        "#     vector_obj.run(string)\n",
        "\n",
        "#     vectors = vector_obj.vectors\n",
        "#     for vec in vectors :\n",
        "#         sentence_vector.append(vec)\n",
        "#     return sentence_vector\n",
        "\n",
        "\n",
        "# def label_encoding(classes):\n",
        "#     cls = []\n",
        "#     cls.append(classes)\n",
        "#     le = LabelEncoder()\n",
        "#     return le.fit_transform(cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mtv9xA1llbMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0cfcae-866f-401f-847d-966cb70e03a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250365\n",
            "27818\n"
          ]
        }
      ],
      "source": [
        "#from Dataset import NewsSentenceLevelClassificationDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "training_data = NewsSentenceLevelClassificationDataset(data_path=\"/content/drive/MyDrive/NEWS Classification/new_embedding.pkl\",label_path=\"/content/drive/MyDrive/NEWS Classification/label.pkl\",num=0.9,mode=\"train\")\n",
        "testing_data = NewsSentenceLevelClassificationDataset(data_path=\"/content/drive/MyDrive/NEWS Classification/new_embedding.pkl\",label_path=\"/content/drive/MyDrive/NEWS Classification/label.pkl\",num=0.1,mode=\"train\")\n",
        "train_dataloader = DataLoader(training_data, batch_size=256, shuffle=True)\n",
        "test_dataloader = DataLoader(testing_data,batch_size=256,shuffle=True)\n",
        "#test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfiNI8owmJpV",
        "outputId": "844ddde4-57c6-4ece-9b27-32ebad11f4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "zDp4YeYelbMV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NewsClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self,num_classes):\n",
        "        super(NewsClassificationModel, self).__init__()\n",
        "        self.lstm =  nn.LSTM(input_size=768,hidden_size=64, bidirectional=True,batch_first=True)\n",
        "        self.dense = nn.Linear(64*2,num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.lossfn = nn.CrossEntropyLoss()\n",
        "    def forward(self, x):\n",
        "        x,_ = self.lstm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self,data,label):\n",
        "        prediction =self.forward(data)\n",
        "#         print(prediction.shape,label.shape)\n",
        "#         label = label.type(torch.FloatTensor)\n",
        "        return prediction,self.lossfn(prediction,label)\n",
        "\n",
        "    def predict(self,data):\n",
        "        return self.forward(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_rGVmMGlbMV",
        "outputId": "7472bf3f-ead5-44b2-c98d-80cecb7f934f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewsClassificationModel(\n",
            "  (lstm): LSTM(768, 64, batch_first=True, bidirectional=True)\n",
            "  (dense): Linear(in_features=128, out_features=27, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (softmax): Softmax(dim=1)\n",
            "  (lossfn): CrossEntropyLoss()\n",
            ")\n",
            "cuda\n",
            "Epoch 0\n",
            "Epoch 1  Training Loss: 3.064839682890498  F1-score tensor(0.3439) Validation Loss: 2.774061769520471 F1-score tensor(0.6422)\n",
            "Epoch 1\n",
            "Epoch 2  Training Loss: 2.891113246814198  F1-score tensor(0.5151) Validation Loss: 2.6503211489511194 F1-score tensor(0.7574)\n",
            "Epoch 2\n",
            "Epoch 3  Training Loss: 2.849776311214626  F1-score tensor(0.5317) Validation Loss: 2.6208725281811636 F1-score tensor(0.7653)\n",
            "Epoch 3\n",
            "Epoch 4  Training Loss: 2.835982854802782  F1-score tensor(0.5372) Validation Loss: 2.6070793440582554 F1-score tensor(0.7711)\n",
            "Epoch 4\n",
            "Epoch 5  Training Loss: 2.8280980297481673  F1-score tensor(0.5409) Validation Loss: 2.598710932862868 F1-score tensor(0.7765)\n",
            "Epoch 5\n",
            "Epoch 6  Training Loss: 2.822620891055166  F1-score tensor(0.5445) Validation Loss: 2.5935660370992957 F1-score tensor(0.7769)\n",
            "Epoch 6\n",
            "Epoch 7  Training Loss: 2.8188087764747007  F1-score tensor(0.5466) Validation Loss: 2.588819742202759 F1-score tensor(0.7810)\n",
            "Epoch 7\n",
            "Epoch 8  Training Loss: 2.815738119109574  F1-score tensor(0.5486) Validation Loss: 2.585621028865149 F1-score tensor(0.7834)\n",
            "Epoch 8\n",
            "Epoch 9  Training Loss: 2.8134158780625245  F1-score tensor(0.5500) Validation Loss: 2.58273016859632 F1-score tensor(0.7849)\n",
            "Epoch 9\n",
            "Epoch 10  Training Loss: 2.811130374094271  F1-score tensor(0.5518) Validation Loss: 2.580180334388663 F1-score tensor(0.7862)\n",
            "Epoch 10\n",
            "Epoch 11  Training Loss: 2.8079813447915423  F1-score tensor(0.5557) Validation Loss: 2.5803804791301763 F1-score tensor(0.7882)\n",
            "Epoch 11\n",
            "Epoch 12  Training Loss: 2.7991443147545247  F1-score tensor(0.5666) Validation Loss: 2.577380974358375 F1-score tensor(0.7887)\n",
            "Epoch 12\n",
            "Epoch 13  Training Loss: 2.793289252125899  F1-score tensor(0.5719) Validation Loss: 2.575527718307775 F1-score tensor(0.7895)\n",
            "Epoch 13\n"
          ]
        }
      ],
      "source": [
        "#from Classifier.Model.SentenceClassification import SentenceClassification\n",
        "import torch\n",
        "from  torch.optim import AdamW\n",
        "#model intialization\n",
        "from torchmetrics.classification import F1Score\n",
        "from tqdm import tqdm\n",
        "f1 = F1Score(task=\"multiclass\", num_classes=27)\n",
        "\n",
        "def init():\n",
        "    model = NewsClassificationModel(27)\n",
        "    return model\n",
        "\n",
        "def training_step(model,data,label):\n",
        "    model.train()\n",
        "    prediction,loss = model.loss(data,label)\n",
        "    data = data.detach().cpu().numpy()\n",
        "    label = label.detach().cpu().numpy()\n",
        "    prediction = prediction.detach().cpu().numpy()\n",
        "    prediction_map = []\n",
        "    label_map = []\n",
        "    for i  in range(len(prediction)):\n",
        "        prediction_index = np.argmax(prediction[i])\n",
        "        label_index = np.argmax(label[i])\n",
        "        prediction_map.append(prediction_index)\n",
        "        label_map.append(label_index)\n",
        "    prediction_map = np.array(prediction_map,dtype=float)\n",
        "    label_map = np.array(label_map,dtype=float)\n",
        "    prediction_map = torch.from_numpy(prediction_map)\n",
        "    label_map = torch.from_numpy(label_map)\n",
        "    f1_score = f1(prediction_map,label_map)\n",
        "\n",
        "    return loss,f1_score\n",
        "\n",
        "def validation_step(model,data,label):\n",
        "    model.eval()\n",
        "    prediction,loss =model.loss(data,label)\n",
        "    data = data.detach().cpu().numpy()\n",
        "    label = label.detach().cpu().numpy()\n",
        "    prediction = prediction.detach().cpu().numpy()\n",
        "    prediction_map = []\n",
        "    label_map = []\n",
        "    for i  in range(len(prediction)):\n",
        "        prediction_index = np.argmax(prediction[i])\n",
        "        label_index = np.argmax(label[i])\n",
        "        prediction_map.append(prediction_index)\n",
        "        label_map.append(label_index)\n",
        "    prediction_map = np.array(prediction_map,dtype=float)\n",
        "    label_map = np.array(label_map,dtype=float)\n",
        "    prediction_map = torch.from_numpy(prediction_map)\n",
        "    label_map = torch.from_numpy(label_map)\n",
        "#     print(prediction_map,label_map)\n",
        "    f1_score = f1(prediction_map,label_map)\n",
        "    return loss, f1_score\n",
        "\n",
        "#begining\n",
        "model = init()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "print(device)\n",
        "optimizer = AdamW(model.parameters(),lr=1e-4)\n",
        "\n",
        "\n",
        "epochs = 150\n",
        "\n",
        "for epoch in  range(0,epochs):\n",
        "    print('Epoch',epoch)\n",
        "    loss_train = []\n",
        "    score_train = []\n",
        "\n",
        "    for data,label in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        label = label.type(torch.FloatTensor)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        loss,f1_score = training_step(model,data,label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        loss_train.append(loss.item())\n",
        "        score_train.append(f1_score)\n",
        "\n",
        "    for data,label in test_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        label = label.type(torch.FloatTensor)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        loss,f1_score = training_step(model,data,label)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        loss_train.append(loss.item())\n",
        "        score_train.append(f1_score)\n",
        "\n",
        "    loss_val =[]\n",
        "    score_val = []\n",
        "    for data,label in test_dataloader:\n",
        "\n",
        "        label = label.type(torch.FloatTensor)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        loss,f1_score = validation_step(model,data,label)\n",
        "        loss_val.append(loss.item())\n",
        "        score_val.append(f1_score)\n",
        "    loss_val = sum(loss_val)/len(loss_val)\n",
        "    loss_train = sum(loss_train)/len(loss_train)\n",
        "    score_train = sum(score_train)/len(score_train)\n",
        "    score_val = sum(score_val)/len(score_val)\n",
        "    torch.save({\"model\":model.state_dict(),\n",
        "                            \"o_model\" : model,\n",
        "                             \"optimizer\":optimizer.state_dict(),\n",
        "                             \"f1-score_training\":  score_train,\n",
        "                             \"f1-score_val\":score_val,\n",
        "                             \"loss_train\":loss_val,\n",
        "                             \"loss_val\" : loss_train\n",
        "                           },'/content/drive/MyDrive/NEWS Classification/Models/Model-Layer-Stopping-Epoch:'+str(epoch+1)+'.pt')\n",
        "                  # print(\"Early stopping called and stopped the training.\")\n",
        "    print(\"Epoch\",(epoch+1),' Training Loss:', loss_train, ' F1-score',score_train,'Validation Loss:',loss_val,'F1-score',score_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fDipEBplbMW"
      },
      "outputs": [],
      "source": [
        "xx!  pip install torch==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4xdzbGfmlbMW",
        "outputId": "b443cc88-9dc5-47c9-c835-9c658f0663a0"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-60f3c2c555b5>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# df = df.sample(n=num)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-60f3c2c555b5>\u001b[0m in \u001b[0;36msentence_embedding\u001b[0;34m(text_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvector_obj\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msentence_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mvector_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mjump\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sent2vec/vectorizer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sentences, remove_stop_words, add_stop_words)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'All items must be string type but {sentence} is type {type(sentence)}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# RUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_stop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sent2vec/vectorizer.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, sentences, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2177\u001b[0m             )\n\u001b[1;32m   2178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#sentence embedding code\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "jump = 256\n",
        "def sentence_embedding(text_list):\n",
        "  f = open(\"embedding.pkl\",\"ab\")\n",
        "  for i in range(0,len(text_list),jump):\n",
        "    vector_obj =Vectorizer()\n",
        "    sentence_vector = []\n",
        "    vector_obj.run(text_list[i:i+jump])\n",
        "\n",
        "    vectors = vector_obj.vectors\n",
        "    for vec in vectors :\n",
        "        sentence_vector.append(vec)\n",
        "    # sentence_vector\n",
        "    print(\"Dumped\",i, i+jump)\n",
        "    pickle.dump(sentence_vector,f)\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/NEWS Classification/preprocessed_data.csv\")\n",
        "df = df.dropna()\n",
        "# df = df.sample(n=num)\n",
        "title = list(df['title'])\n",
        "sentence_embedding(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0Crg9zq2qnPF"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "import pickle\n",
        "import pandas as pd\n",
        "# label_encoder object knows\n",
        "# how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/NEWS Classification/preprocessed_data.csv\")\n",
        "df = df.dropna()\n",
        "classes= label_encoder.fit_transform(df[\"class\"])\n",
        "classes = np.array(classes)\n",
        "f = open(\"/content/drive/MyDrive/NEWS Classification/label.pkl\",\"wb\")\n",
        "pickle.dump(classes,f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/drive/MyDrive/NEWS Classification/new_embedding.pkl\",\"rb\")\n",
        "fb = open(\"/content/drive/MyDrive/NEWS Classification/label.pkl\",\"rb\")\n",
        "result =pickle.load(f)\n",
        "result1 = pickle.load(fb)\n",
        "\n",
        "print(len(result),result1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGYcCqcVsWxQ",
        "outputId": "35c45e05-09e9-4bc0-9c48-5c36be326571"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "278184 (278184,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_qFx2KmMtNsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}